ransha
Ran Shaham (203781000)
EX: 3

FILES:
README			-- This file
MapReduceFramework.cpp	-- My implementation of the uthreads library.
Search.cpp		-- The test (Task 1) for the library.
Makefile		-- Creates the library and 'Search' when called with 
			   no arguments
my_pthread.h		-- Wrapper for some of the pthread functions to handle
			   unsuccessful pthread calls
error_handle.h		-- A header that includes the function that handles
			   errors in this exercise
RR.jpg			-- Gantt chart for question 6 - Round Robin
FCFS.jpg		-- " - First Come First Serve
SRTF.jpg		-- " - Shortest Remaining Time First
Priority.jpg		-- " - Priority Scheduling


REMARKS:
Framework Design:
Most of the design was driven directly from the exercise description, except
for the following:
* The ExecMap output data structures are vectors, which allow the shuffle to
  read from them while the map writes (push_back) to them.
* Synchronization between the map and the shuffle is done by signaling the
  shuffle thread when a new chunk is written.
* The shuffle keeps a vector of indices. Each entry specifies the last index
  in each ExecMap vector that has been written, and DOES NOT pop items from
  the map's vector. After reading, it updates the index for that container.

Search.cpp Design:
First I'll describe the k1-k3,v1-v3 types I chose-
* The <k1,v1> pairs are the directory names and the substring to search
  respectively. This way, on each call to the Map function, the given directory
  is opened (it is written in the forums that we can assume that each folder
  will appear only once in the arguments), iterated over and searched for file
  names that contain the substring.
* The k2 is the filename that contains the substring. That is, a class that
  has a string member that holds the filename. The v2's only purpose is to
  free allocated k2 pointers. It takes a k2* as an argument and when destructed
  it also deletes the k2 that it holds. The idea is that every v2 created has
  its own k2, and the shuffle can "ignore" (merge) several <k2,v2> pairs to:
  <k2, list<v2>> pairs - i.e. no v2 is ever ignored. Therefore when the v2s
  are destroyed, all their k2s can be safely destroyed, and that happens
  implicitly when calling to "delete v2" (destructor). This way all of the 
  <k2,v2> allocated memory is freed.
* k3 is the same as k2 (filename), and v3 is simply the number of times the
  file name that is held in k3 appeared in the input directories. For example,
  if the directory structre is this:

  root	-> dir1	-> a, b, c, .vimrc
	-> dir2	-> d, a, e
	-> dir3	-> Makefile, README, test.o, a
	-> dir4	-> .ssh, .config, Games

  then calling "Search a dir1 dir2 dir3" will yield [after reduce] the <k3,v3>
  entry: <"a",3> (among the other entries - <"e",1>, <"b",1> etc.).
Next, the Map/Reduce operations:
* Map - Gets a <dirname, strToFind> pair. It checks whether the 'dirname' is a
  valid directory (and if it exists) - if not it returns. Otherwise, it opens
  the directory and for each entry (file/link/dir) checks if the 'strToFind'
  is a substring of that entry's name. If so, it creates the <filename, v2>
  pair using (using the 'new' operator) and sends it to Emit2.
* Reduce - Gets a <filename, list<v2>>  pair. It copies the filename to the
  k3 entry, and sets the value of v3 (filename count) to the size of the list
  of v2s. This works because the shuffle merges all files with the same name
  while keeping the v2s in a list for that filename, so the lise size is
  exactly the number of occurunces of the filename.
  Then, it iterates over the input list and delete the v2* values (and k2,
  implicitly), since they are no longer needed for the framework.
Having said that, the Search procedure is as follows:
	1. Prepare the input - convert the input string arguments to the 
	   string to search and the directory names, and create a list of
	   <k1*, v1*> pairs.
	2. Run the framework and get the output <filename,count> list.
	3. For each list entry of <file_i, count_i>, print file_i count_i
	   times.
	4. Iterate over the input and output lists and delete allocated memory

ANSWERS:

Q1:
The program will work with multiple processes (instead of threads).
Each process
* The process using select is the Shuffle, since it waits for data to be
  written 

Q2:
Since the user's computer can run 8 threads concurrently - 1 on each core,
multiThreadLevel should be at least 7 which will yield 8 threads running at
the same time: 7 ExecMap threads + 1 shuffle thread.
Since the computer doesn't support hyper-threading, any additional thread will
wait till a thread that was created earlier will finish running, therefore it
is redundant. E.g., say multiThreadLevel is 10, then 8 ExecMap threads are
created and being run and the remaining 2 WAIT until 2 of the earlier ones
finish running. When they do, by definition there is no more mapping to be
done so they are immidiately terminated. Then, after the mapping is finished
can a shuffle be created and only start shuffling.
On the other hand, less than 7 won't utilze all system resources - after
creating 4 (for example) ExecMap threads the main thread then creates one 
shuffle thread and waits for them to finish. So there will be 5 working thread
untill the shuffle is done and 2 cores will remain unused this entire time
(lower CPU utilization).
In conclusion, I'd suggest to set multiThreadLevel to 7.

Q3:
a. Nira's solution - Single thread
* Does not utilze multi-cores since only one thread of execution is running.
* No scheduler is needed since no concurrency is happening, the program flow
  is serial.
* No communication is needed since there's a single thread and process.
* When waiting for I/O operations or other time consuming actions, the program
  does not progress
* The overall speed is probably slower than the other methods, since the
  process can spend a lot of time waiting (for disk access when writing the 
  log, for example) and cannot do anything in this time.

b. Moti - POSIX's library
* Utilizes multi-cores since the OS is aware of kernel-level threads and can
  run a thread for each core or processor concurrently.
* The library manages its own scheduling methods so besides block / signal
  there is no much scheduling that can be done.
* Communication time is relatively fast since the threads share heap and static
  data segments, and can read from them concurrently. Writing requires locking
  shared resources, but it is still better than multi-process communication.
* When one thread is blocked, another can proceed, although when accessing disk
  no concurrency can occur (that is, two threads cannot acess the disk in the
  same time).
* The overall speed depends on whether the machine running the framework has
  multiple cores or not. If not, there is no advantage for this method over
  user-level threads, and the overhead of context switching, thread creation
  and termination is bigger than in user-level threads.
  If the machine has multiple cores or processes, this method allows more than
  one thread running in the same time, thus is probably faster than the other
  methods.

c. Danny - User-level threads
* Since the OS is not aware of the multiple threads running, it does not
  run threads on multiple cores concurrently, so this method does not utilize
  multiple cores.
* This is THE method for creating scheduler based on internal data - the user
  can do whatever he/she wants, and has access to all data in the program, so
  this method is very flexible in terms of scheduling.
* Communication time is similar to kernel-level threads. If no built-in OS
  mutex types are used (which gurantee atomic operations) synchronization can
  be more tricky than in kernel-level.
* The OS is not aware of the fact multiple threads are running, so when the
  process makes a system call, the OS is likely to block it, therefore not
  allowing other user-level threads to run in the meantime. If the OS does
  not block the entire process, then the program can progress on another
  thread while another one waits.
* This is likely to be a fast implementation on single core machines, since
  context switches and thread actions require mainly function calls, and not
  system calls, therefore overhead time is reduced. On multiple cores, no
  concurrency is involved so this method is likely to be slower than methods
  that enable concurrency (multi-process or kernel-level threads).

d. Galit - Multi-processes
* This method utilizes multi-cores since modern OSs tend to do so with
  multiple processes.
* The scheduling is almost entirely up to the OS to decide - there are some
  hints that can affect the scheduler (nice values, for example) but are based
  on general process role, and not fine-grained internal data.
* Communication is trickier than in other methods - the processes need pipes
  or files to communicate and handling files require sys-calls, which are slow
  as we measured in EX1.
* Each process is more or less independent, so while one is blocked another can
  proceed (if it should, logically).
* In my opinion (i.e - based on my knowledge so far, not experience) this
  method will be slower than the others on single core machines, and perhaps
  a bit faster on multi-core than the single-thread or user-level threads 
  methods, but slower than the kernel-level threads.

Q4:
Processes:
a. Stack is not shared, each process gets its own stack.
b. Same for the heap.
c. Same for global variables.
When using fork() sys call, the above data segments are copied to the child
process, not shared.

Kernel-level threads:
a. Each thread gets its own stack
b. Threads share heap data, since they reside in the same process
c. Same as heap, the process' static data segment is the same for all threads

User-level threads:
Exactly the same as ernel-level threads.
A stack can't be shared between threads since they run different functions and
execute a different set of commands (perhaps), so each thread should have its
own stack for local variables and adresses.
The heap and static data segments are shared, since the threads are run in a
one process.

Q5:
A deadlock is a situation in which two or more concurrent running threads of
execution (processes/ threads) try to access some shared resources; each of
them blocks another and is blocked by another, thus neither of them
progresses with their execution.
For example, threads A,B both need resources x,y to perform an operation.
Consider the following scenerio: A runs, reserves x; then, a context switch
occurs and B reserves y. If it tries to reserve x it will be blocked. If
A tries to reserve y it will also be blocked. Therefore they both block each
other and neither progresses.

A livelock is also a situation in which two or more running threads don't
proceed with their execution. In this case, all threads are taking active
actions in order to let the other threads finish their work, and then proceed.
If each thread lets the other threads proceed and no thread actually proceeds,
a livelock occurs.
For example, say threads A,B need resources x,y as before. Again, A reserves x
and B reserves y. Now B tries to reserve x and fails so it lets go of y and
waits 1 second. Before it does, A gets the control and tries to access y, 
which is still in B's control, so it lets go of x and waits for 1 second.
Then, they try again using the same method. This creates the above scenerio,
where 2 threads aren't progressing but both are taking active actions to
prevent locks.

Q6:
# NAME	| Avg. wait time (format: (P1:a+b+... + P2:c+d+... + ...) / 5 = result
======	| =================
1. RR	| (1+2+3+2 + 1 + 2 + 2+3+2 + 3) / 5 = 4.2
=======	|
2. FCFS	| (0 + (10-1) + (11-3) + (13-7) + (25-8)) / 5 = 8
=======	|
3. SRTF	| (1+2+1 + 0 + 0 + 7 + 0) / 5 = 2.2
=======	|
4. PRIO	| (0 + (10-1) + (23-3) + (11-7) + (25-8)) / 5 = 10
===========================
# NAME	| Turnaround time
=======	| =================
1. RR	| (18 + (3-1) + (7-3) + (26-7) + (12-8)) / 5 = 9.4
=======	|
2. FCFS	| (10 + (11-1) + (13-3) + (25-7) + (26-8)) / 5 = 13.2
=======	|
3. SRTF	| (14 + (2-1) + (5-3) + (26-7) + (9-8)) / 5 = 7.4
=======	|
4. PRIO	| (10 + (11-1) + (25-3) + (23-7) + (26-8)) / 5 = 15.2

* (All time calculations are ignoring context switching)
